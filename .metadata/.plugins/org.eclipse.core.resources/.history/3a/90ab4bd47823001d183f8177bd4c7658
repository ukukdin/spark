package Spark_Scala

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD.rddToPairRDDFunctions

object CacheSize {
  
  def main(args: Array[String]): Unit = {
    
		  val conf = new SparkConf()
				  .setAppName("cachesize")
				  .setMaster("local")
				  
				  val sc = new SparkContext(conf)
				  
				  val intRdd = sc.parallelize(1 to 10000)
				  intRdd.name = "intRdd"
				  println(intRdd.cache)
				  println(intRdd.count)
				  val strRdd = intRdd.map(_.toString)
				  strRdd.name = "strRdd"
				  println(strRdd.cache)
				  println(strRdd.count)
				  case class strCase (str: String)
				  val strCaseRdd = intRdd.map(x => strCase(x.toString))
				  strCaseRdd.name = "strCaseRdd"
				  println(strCaseRdd.cache)
				  println(strCaseRdd.count)
				  //웹 ui 확인 (storage탭에서 캐시 size 확인) object (header 16 bytes), String(overhead 40 bytes, 2 bytes/char)
				  
				  
  }
  
}