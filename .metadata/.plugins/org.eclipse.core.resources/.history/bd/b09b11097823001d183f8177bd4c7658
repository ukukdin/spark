package Spark_Scala

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD.rddToPairRDDFunctions

object CacheSize {
  
  val conf = new SparkConf()
					.setAppName("partF")
					.setMaster("local")

					val sc = new SparkContext(conf)
  
  			val intRdd = sc.parallelize(1 to 10000)
  			intRdd.name = "intRdd"
  			print(intRdd.cache)
  			print(intRdd.count)
  			val strRdd = intrdd.map(_.tostring)
  			strRdd.name = "strRdd"
  			print(strRdd.cache)
  			print(strRdd.count)
  
}